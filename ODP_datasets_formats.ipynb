{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformation of the data available in the Google sheet \"ODP OPENNESS INDICATOR_local\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## connect to google drive API and read the data in the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " gspread [makes possible to access google sheets](https://gspread.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm.notebook import tnrange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_google_drive_access_scope():\n",
    "    \"\"\"\n",
    "    define the scope of the access to google drive and create credentials using the privatepprint.json file)\n",
    "    \"\"\"\n",
    "    \n",
    "    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('dataset_types_ODP.json', scope)\n",
    "\n",
    "    client = gspread.authorize(creds)\n",
    "    \n",
    "    return client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the data from the google sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_google_sheet(sheet_name, worksheet_name, client):\n",
    "    \"\"\"\n",
    "    getting access to the google sheet spreadsheet and the name of the \"History\" sheet we want to access\n",
    "    \"\"\"\n",
    "    \n",
    "    sheet = client.open(sheet_name) # access the google sheet\n",
    "    history = sheet.worksheet(worksheet_name) # access the worksheet \"History\"\n",
    "    \n",
    "    # extract all the raw data available\n",
    "    data_types = history.get_all_records()\n",
    "    \n",
    "    return data_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tranforming the data into tabular data and making some cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_into_dataframe(data):\n",
    "    \"\"\"\n",
    "    transform the dict into a padas\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_data = pd.DataFrame(data)    \n",
    "    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making some data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_not_used_columns(data):\n",
    "    \"\"\"\n",
    "    drop not required columns: openess indicator label, openess indicator value, total datasets label, total datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    return data.drop([\"2\", \"3\", \"4\", \"5\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processing each row of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_row(row_to_process, data):\n",
    "    \"\"\"\n",
    "    extract the information correspondig to a specific row for reshape it properly: <date>, <data type>, <number of datasets>\n",
    "    \"\"\"   \n",
    "    data_row = data.loc[row_to_process]\n",
    "    \n",
    "    #create a dataframe with the information extacter for each row. It will make further reshaping easier\n",
    "    return pd.DataFrame(data_row).T.reset_index().drop([\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_removing_empty_columns(data_row):\n",
    "    \"\"\"\n",
    "    remove the columns having no values. Here no value is represented by ''\n",
    "    \"\"\"\n",
    "\n",
    "    return data_row.drop([col for col in data_row if (data_row[col] == '').any()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_identification(data_row):\n",
    "    \"\"\"\n",
    "    extract the columns corresponding to the data types and the number of datasets per data type\n",
    "    \"\"\"\n",
    "\n",
    "    data_row_type_labels = data_row.columns[1:data_row.shape[1]:2]\n",
    "    data_row_value_labels = data_row.columns[2:data_row.shape[1]:2]\n",
    "    \n",
    "    return data_row_type_labels, data_row_value_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_separation(data_row, data_row_type_labels, data_row_value_labels):\n",
    "    \"\"\"\n",
    "    create 2 dataframes: 1 containing the data set type and 1 containing the value of the corresponding dataset type and reshape as row-oriented by transposing it\n",
    "    \"\"\"\n",
    "    # dataset type columns extraction\n",
    "    data_row_type = data_row[data_row_type_labels].T.reset_index()\n",
    "    data_row_type = data_row_type.drop([\"index\"], axis=1).rename(columns={0:\"dataset type\"}) # rename the column name\n",
    "    \n",
    "    # number of dataset types columns extraction\n",
    "    data_row_value = data_row[data_row_value_labels].T.reset_index()\n",
    "    data_row_value = data_row_value.drop([\"index\"], axis=1).rename(columns={0:\"number of datasets\"}) # rename the column name\n",
    "    \n",
    "    return data_row_type, data_row_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset_types_description(data_row_type):\n",
    "    \"\"\"\n",
    "    clean the row_data_type by removing unwanted text: text before the \"/\" character. It also removes any rows with 'None' value\n",
    "    \"\"\"\n",
    "\n",
    "    data_row_type = data_row_type.dropna() #remove rows having 'None' values\n",
    "    data_row_type_cleaned = data_row_type[\"dataset type\"].apply(lambda x: x.lower()).apply(lambda x: x.split(\"/\"))\n",
    "    data_row_type_cleaned = pd.DataFrame(data_row_type_cleaned)\n",
    "    data_row_type_cleaned = data_row_type_cleaned[\"dataset type\"].apply(lambda x: x[-1])\n",
    "    \n",
    "    return pd.DataFrame(data_row_type_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(data_row_type, data_row_value):\n",
    "    \"\"\"\n",
    "    concatenate the 2 created dataframes into 1 that will serve in the final datamodel for Qlik Sense\n",
    "    \"\"\"\n",
    "    \n",
    "    data_row_transformed = pd.concat([data_row_type, data_row_value], axis=1, ignore_index=True)\n",
    "\n",
    "    # rename the columns\n",
    "    data_row_transformed.rename(columns={0:\"dataset type\", 1:\"number of datasets\"}, inplace=True)\n",
    "    \n",
    "    return data_row_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# equivalences table for removing duplicated datasets: same dataset type, different names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_equivalences_table():\n",
    "    \"\"\"\n",
    "    table creation by manually entry the possible datasets types writting: added as founded in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    data_types_equivalance = {\"comma-separated-values\":\"csv\", \"sparql-query\":\"sparql\", \"tab-separated-values\":\"tsv\", \"pdf;type=pdf1x\":\"pdf\", \"rdf+xml\":\"rdf_xml\"}\n",
    "    \n",
    "    return data_types_equivalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_replace_equivalent_dataset_name(data_row_transformed, data_types_equivalance):\n",
    "    \"\"\"\n",
    "    find and replace dataset types names to remove equivalent names for the same types of datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    data_row_transformed.replace(data_types_equivalance, inplace=True)\n",
    "    \n",
    "    return data_row_transformed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_grouping(data_row_transformed):\n",
    "    \"\"\"\n",
    "    removing duplicated data: group by dataset type and summing up the number of datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    return data_row_transformed.groupby([\"dataset type\"]).sum().reset_index().rename(columns={1:\"number of datasets\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add the date column to finalise the datamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hours(data_row):\n",
    "    \"\"\"\n",
    "    remove the hours minutes and seconds in the date dimention\n",
    "    \"\"\"\n",
    "\n",
    "    date_object = datetime.datetime.strptime(data_row[\"1\"][0], '%m/%d/%Y %H:%M:%S').date()\n",
    "\n",
    "    # transform the datetime object into a string with format dd/mm/yyy\n",
    "    date = date_object.strftime('%d/%m/%Y')\n",
    "\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_proccesed_dates(dates_proccesed, current_date):\n",
    "        \"\"\"\n",
    "        create a list of already processed dates from the google sheet file containing the data source. This list will be saved on a file for reuse when updating the processed data\n",
    "        \"\"\"\n",
    "        \n",
    "        return dates_proccesed.append({\"processed dates\": current_date}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_change_detector(data_cleaned, current_date): \n",
    "    \"\"\"\n",
    "    detect a month change in the dataset and add a flag to the rows of the last day of each month (flag = 1) otherwise, there is not flag (flag = 0).\n",
    "    \n",
    "    The current dataset exhibits a non-continuity of the dates, there are many days missing.\n",
    "    \"\"\"\n",
    "      \n",
    "    # extract the previous date to check if the month has changed\n",
    "    past_date = date_tracking.loc[current_row - 1][0]    \n",
    "\n",
    "    # transform the dates into a datetime object\n",
    "    past_date_datetime = datetime.datetime.strptime(past_date, \"%d/%m/%Y\")\n",
    "    current_date_datetime = datetime.datetime.strptime(current_date, \"%d/%m/%Y\")\n",
    "\n",
    "    # extract the month of the date\n",
    "    past_month = past_date_datetime.month\n",
    "    current_month = current_date_datetime.month\n",
    "\n",
    "    # detect month change\n",
    "    if current_month != past_month:\n",
    "        flag_month_change = 1\n",
    "    else:\n",
    "        flag_month_change = 0\n",
    "        \n",
    "    return flag_month_change, past_date, current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_formatting(length_data_row, date):\n",
    "    \"\"\"\n",
    "    create a dataframe with the date having equal length that row_data_type_labels and  row_data_value_labels\n",
    "    \"\"\"\n",
    "    \n",
    "    date_list = [[date, 0] for i in range(length_data_row)]\n",
    "           \n",
    "    return pd.DataFrame(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_data_merge(df_date, data_row_transformed):\n",
    "    \"\"\"\n",
    "    for the current processing row: add the date colum to the dataframe containing the cleaned version of the data types and the dataset number per type\n",
    "    \"\"\"\n",
    "\n",
    "    data_row_transformed = pd.concat([df_date, data_row_transformed], axis=1, ignore_index=True)\n",
    "\n",
    "    # rename the columns, sort values by number of datasets type and remove a self-created \"index\" column\n",
    "    data_row_transformed.rename(columns={0:\"date\", 1:\"last day of the month\", 2:\"dataset type\", 3:\"number of datasets\"}, inplace=True)\n",
    "    data_row_transformed = data_row_transformed.sort_values(by=[\"number of datasets\"], ascending=False).reset_index().drop([\"index\"], axis=1)\n",
    "    \n",
    "    return data_row_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_flag_for_month_change(data_processed, past_date):\n",
    "    \"\"\"\n",
    "    add a flag in case of month change detected - month_change_flag = 1    \n",
    "    \"\"\"\n",
    "    \n",
    "    # add a flag=1 to a previous date if there is month change\n",
    "    data_processed.loc[data_processed[\"date\"] == past_date, \"last day of the month\"] = 1\n",
    "    \n",
    "    return data_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the data and the last update info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_and_info_update(data_processed, date_processed): \n",
    "    \"\"\"\n",
    "    save the processed data into a CSV file: datasets_formats_processed.csv - headers included\n",
    "    save the processed dates for start next updates from this position and not the entire data file. It stores the information into a CSV file: processed_dates_update.csv - headers included\n",
    "    \n",
    "    IMPORTANT: put attention to the date format in the \"processed_dates_update.csv\" file. The format doesn't match the format used in google sheets. You will need to change the date format in this file before\n",
    "                going for updates\n",
    "    \"\"\"\n",
    "\n",
    "    # save the processed data into a CSV file with headers\n",
    "#     file_path_data = \"C:\\\\Users\\\\carlo\\\\Dropbox\\\\Programming\\\\Python\\\\dataset types ODP\\\\datasets_formats_processed.csv\"  # my office laptop\n",
    "    file_path_data = \"D:\\\\Dropbox\\\\Programming\\\\Python\\\\datasets files formats ODP\\\\datasets_formats_processed.csv\"  # my home laptop\n",
    "    data_processed.to_csv(file_path_data, index=False, header=True)\n",
    "    \n",
    "    # save the last row processed info (google sheet row number and last date present in this row) into a CSV file with headers\n",
    "#     file_path_update = \"C:\\\\Users\\\\carlo\\\\Dropbox\\\\Programming\\\\Python\\\\dataset types ODP\\\\processed_dates_update.csv\"  # my office laptop\n",
    "    file_path_update =  \"D:\\\\Dropbox\\\\Programming\\\\Python\\\\datasets files formats ODP\\\\processed_dates_update.csv\"  # my home laptop\n",
    "    date_processed.to_csv(file_path_update, index=False, header=True)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> main function <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of the imported data is: (1000, 104)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001aeada81274675b4743ae45e106c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='data rows processing', max=1000.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# global variables\n",
    "sheet = \"ODP OPENNESS INDICATOR_local_4\"\n",
    "worksheet = \"History\"\n",
    "\n",
    "# set connection to google drive\n",
    "google_client = set_google_drive_access_scope()\n",
    "\n",
    "# acquire the data\n",
    "data_types = access_google_sheet(sheet, worksheet, google_client)\n",
    "raw_data = transform_into_dataframe(data_types)\n",
    "\n",
    "total_rows = raw_data.shape[0]\n",
    "print(f'the size of the imported data is: {raw_data.shape}\\n')\n",
    "\n",
    "# clean the data\n",
    "data_cleaned = drop_not_used_columns(raw_data)\n",
    "\n",
    "# load the table containing equivalent names for the same dataset. It'll be used to have the same name for the same dataset type\n",
    "datasets_type_equivalences = load_equivalences_table()\n",
    "\n",
    "# process all the rows of the dataset\n",
    "# total_rows = 10\n",
    "for current_row in tqdm(range(total_rows), desc=\"data rows processing\"): #tqdm_notebook\n",
    "    # process each row\n",
    "    row_data = extract_row(current_row, data_cleaned)\n",
    "\n",
    "    # remove empty columns in the extracted row\n",
    "    row_data_clean = row_removing_empty_columns(row_data)\n",
    "\n",
    "    # identification of the columns related to the dataset formats and the columns related to the number of dataset formats\n",
    "    row_data_type_labels, row_data_value_labels = columns_identification(row_data_clean)\n",
    "\n",
    "    # separation of the columns related to the dataset formats and the columns related to the number of dataset formats\n",
    "    row_data_type, row_data_value = columns_separation(row_data_clean, row_data_type_labels, row_data_value_labels)\n",
    "\n",
    "    # clean the dataset type desciption column by filtering out the dataset type\n",
    "    row_data_type_clean = clean_dataset_types_description (row_data_type)\n",
    "\n",
    "    # merge dataset types and dataset values into a single dataframe\n",
    "    row_data_transformed = merge_columns(row_data_type_clean, row_data_value)    \n",
    "\n",
    "    # find and replace equivalent names for the same dataset type\n",
    "    row_data_transformed_cleaned = find_and_replace_equivalent_dataset_name(row_data_transformed, datasets_type_equivalences)\n",
    "\n",
    "    # combine together same dataset types and sum up the number of datasets per data type\n",
    "    row_data_transformed_cleaned = data_grouping(row_data_transformed_cleaned)\n",
    "\n",
    "    # remove hour info from the date\n",
    "    date_clean = remove_hours(row_data)\n",
    "    \n",
    "    # add the current date to the list of already processed dates\n",
    "    if current_row == 0:\n",
    "        date_tracking = pd.DataFrame({\"processed dates\": [date_clean]})        \n",
    "    else:\n",
    "        date_tracking = track_proccesed_dates(date_tracking, date_clean)\n",
    "        # detect a change of month \n",
    "        month_change_flag, past_date, current_date = month_change_detector(date_tracking, date_clean)      \n",
    "                   \n",
    "    # generate a new date column with a month change flag column ready to add to the cleaned row dataset\n",
    "    df_date = date_formatting(row_data_transformed_cleaned.shape[0], date_clean)\n",
    "    \n",
    "    # add the date info to the dataframe containing he dataset types and the number of dataset types and make some formatting\n",
    "    row_data_final = row_data_merge(df_date, row_data_transformed_cleaned)\n",
    "\n",
    "    # append the processed data\n",
    "    if current_row == 0:\n",
    "        data_processed = row_data_final\n",
    "    else:\n",
    "        data_processed = data_processed.append(row_data_final, ignore_index=True)\n",
    "        # change the \"last day of the month\" column vaue to 0 --> 1\n",
    "        if month_change_flag:\n",
    "            data_processed = add_flag_for_month_change(data_processed, past_date)\n",
    "        \n",
    "# save the processed data into a CSV file\n",
    "save_data_and_info_update(data_processed, date_tracking)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> dev zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c5ea9ed02148ddbe1f97c8dea6b1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='data rows processing', max=1038, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month change flag = 1 previous date: 31/07/2016 current date: 01/08/2016\n",
      "month change flag = 1 previous date: 29/08/2016 current date: 06/09/2016\n",
      "month change flag = 1 previous date: 30/09/2016 current date: 01/10/2016\n",
      "month change flag = 1 previous date: 31/10/2016 current date: 01/11/2016\n",
      "month change flag = 1 previous date: 30/11/2016 current date: 01/12/2016\n",
      "month change flag = 1 previous date: 31/12/2016 current date: 01/01/2017\n",
      "month change flag = 1 previous date: 31/01/2017 current date: 01/02/2017\n",
      "month change flag = 1 previous date: 28/02/2017 current date: 01/03/2017\n",
      "month change flag = 1 previous date: 30/03/2017 current date: 10/04/2017\n",
      "month change flag = 1 previous date: 30/04/2017 current date: 01/05/2017\n",
      "month change flag = 1 previous date: 31/05/2017 current date: 01/06/2017\n",
      "month change flag = 1 previous date: 30/06/2017 current date: 01/07/2017\n",
      "month change flag = 1 previous date: 31/07/2017 current date: 01/08/2017\n",
      "month change flag = 1 previous date: 31/08/2017 current date: 01/09/2017\n",
      "month change flag = 1 previous date: 26/09/2017 current date: 05/10/2017\n",
      "month change flag = 1 previous date: 31/10/2017 current date: 01/11/2017\n",
      "month change flag = 1 previous date: 30/11/2017 current date: 01/12/2017\n",
      "month change flag = 1 previous date: 31/12/2017 current date: 01/01/2018\n",
      "month change flag = 1 previous date: 31/01/2018 current date: 01/02/2018\n",
      "month change flag = 1 previous date: 28/02/2018 current date: 01/03/2018\n",
      "month change flag = 1 previous date: 31/03/2018 current date: 01/04/2018\n",
      "month change flag = 1 previous date: 30/04/2018 current date: 01/05/2018\n",
      "month change flag = 1 previous date: 29/05/2018 current date: 04/06/2018\n",
      "month change flag = 1 previous date: 08/06/2018 current date: 21/10/2018\n",
      "month change flag = 1 previous date: 21/10/2018 current date: 06/02/2019\n",
      "month change flag = 1 previous date: 28/02/2019 current date: 01/03/2019\n",
      "month change flag = 1 previous date: 31/03/2019 current date: 01/04/2019\n",
      "month change flag = 1 previous date: 30/04/2019 current date: 01/05/2019\n",
      "month change flag = 1 previous date: 31/05/2019 current date: 01/06/2019\n",
      "month change flag = 1 previous date: 30/06/2019 current date: 01/07/2019\n",
      "month change flag = 1 previous date: 31/07/2019 current date: 01/08/2019\n",
      "month change flag = 1 previous date: 31/08/2019 current date: 01/09/2019\n",
      "month change flag = 1 previous date: 30/09/2019 current date: 01/10/2019\n",
      "month change flag = 1 previous date: 31/10/2019 current date: 01/11/2019\n",
      "month change flag = 1 previous date: 30/11/2019 current date: 01/12/2019\n",
      "month change flag = 1 previous date: 31/12/2019 current date: 01/01/2020\n",
      "month change flag = 1 previous date: 31/01/2020 current date: 01/02/2020\n",
      "month change flag = 1 previous date: 29/02/2020 current date: 01/03/2020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# process all the rows of the dataset\n",
    "total_rows = 1038 #1038\n",
    "for current_row in tqdm(range(total_rows), desc=\"data rows processing\"): #tqdm_notebook\n",
    "    # process each row\n",
    "    row_data = extract_row(current_row, data_cleaned)\n",
    "\n",
    "    # remove empty columns in the extracted row\n",
    "    row_data_clean = row_removing_empty_columns(row_data)\n",
    "\n",
    "    # identification of the columns related to the dataset formats and the columns related to the number of dataset formats\n",
    "    row_data_type_labels, row_data_value_labels = columns_identification(row_data_clean)\n",
    "\n",
    "    # separation of the columns related to the dataset formats and the columns related to the number of dataset formats\n",
    "    row_data_type, row_data_value = columns_separation(row_data_clean, row_data_type_labels, row_data_value_labels)\n",
    "\n",
    "    # clean the dataset type desciption column by filtering out the dataset type\n",
    "    row_data_type_clean = clean_dataset_types_description (row_data_type)\n",
    "\n",
    "    # merge dataset types and dataset values into a single dataframe\n",
    "    row_data_transformed = merge_columns(row_data_type_clean, row_data_value)    \n",
    "\n",
    "    # find and replace equivalent names for the same dataset type\n",
    "    row_data_transformed_cleaned = find_and_replace_equivalent_dataset_name(row_data_transformed, datasets_type_equivalences)\n",
    "\n",
    "    # combine together same dataset types and sum up the number of datasets per data type\n",
    "    row_data_transformed_cleaned = data_grouping(row_data_transformed_cleaned)\n",
    "\n",
    "    # remove hour info from the date\n",
    "    date_clean = remove_hours(row_data)\n",
    "    \n",
    "    # add the current date to the list of already processed dates\n",
    "    if current_row == 0:\n",
    "        date_tracking = pd.DataFrame({\"processed dates\": [date_clean]})        \n",
    "    else:\n",
    "        date_tracking = track_proccesed_dates(date_tracking, date_clean)\n",
    "        # detect a change of month \n",
    "        month_change_flag, past_date, current_date = month_change_detector(date_tracking, date_clean)\n",
    "                        \n",
    "        if month_change_flag:\n",
    "            print(f'month change flag = {month_change_flag} previous date: {past_date} current date: {current_date}')    \n",
    "                   \n",
    "    # generate a new date column with a month change flag column ready to add to the cleaned row dataset\n",
    "    df_date = date_formatting(row_data_transformed_cleaned.shape[0], date_clean)\n",
    "    \n",
    "    # add the date info to the dataframe containing he dataset types and the number of dataset types and make some formatting\n",
    "    row_data_final = row_data_merge(df_date, row_data_transformed_cleaned)\n",
    "\n",
    "    # append the processed data\n",
    "    if current_row == 0:\n",
    "        data_processed = row_data_final\n",
    "    else:\n",
    "        data_processed = data_processed.append(row_data_final, ignore_index=True)\n",
    "        # change the \"last day of the month\" column vaue to 0 --> 1\n",
    "        if month_change_flag:\n",
    "            data_processed = add_flag_for_month_change(data_processed, past_date)\n",
    "        \n",
    "# save the processed data into a CSV file\n",
    "save_data_and_info_update(data_processed, date_tracking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
