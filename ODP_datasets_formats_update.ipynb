{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import pandas as pd\n",
    "import datetime # import the datetime module\n",
    "from datetime import datetime as dt # import the datetime type\n",
    "from tqdm.notebook import tnrange, tqdm\n",
    "import pathlib\n",
    "import sys\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read file containing the last updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_processed_date(file): \n",
    "    \"\"\"\n",
    "    read the file containing the last dates processed \"processed_dates_update.csv\" and calculate the next line to read in the worksheet. \n",
    "    \"\"\"\n",
    " \n",
    "    last_processed_dates = pd.read_csv(file)\n",
    "\n",
    "    # calculate the starting line to read\n",
    "    return last_processed_dates.shape[0] + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# connect to google drive API and read the data in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_google_drive_access_scope():\n",
    "    \"\"\"\n",
    "    define the scope of the access to google drive and create credentials using the privatepprint.json file)\n",
    "    \"\"\"\n",
    "    \n",
    "    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name('dataset_types_ODP.json', scope)\n",
    "\n",
    "    client = gspread.authorize(creds)\n",
    "    \n",
    "    return client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get all the data between a row range in the google sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_google_sheet(sheet_name, worksheet_name, client, file, first_column, last_column):\n",
    "    \"\"\"\n",
    "    getting access to the google sheet spreadsheet and the name of the \"History\" sheet we want to access\n",
    "    \"\"\"\n",
    "    \n",
    "    sheet = client.open(sheet_name) # access the google sheet\n",
    "    history = sheet.worksheet(worksheet_name) # access the worksheet \"History\"      \n",
    "    \n",
    "    # extract all the raw data available\n",
    "    next_row_to_read = last_processed_date(file)    \n",
    "    \n",
    "    # returns the number of rows and columns in the worksheet by counting the number of non-empty cells in the first column. The first column corresponds to the date.\n",
    "    # this solution should work for at least 5 years and until the number of retrieved rows will be large enough to load in memory. This because we read all the data in the first column\n",
    "    row_total = len(history.col_values(1))     \n",
    "    \n",
    "    # set the batch of data to retrieve\n",
    "    if row_total < next_row_to_read:  # if there are new data to read compared to the last time the data was updated\n",
    "        new_data = [0]        \n",
    "    else:\n",
    "        cell_start = first_column + str(next_row_to_read)\n",
    "        cell_end = last_column + str(row_total)  \n",
    "        new_data = history.batch_get([cell_start + \":\" + cell_end])\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tranforming the data into tabular data and making some cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_into_dataframe(data):\n",
    "    \"\"\"\n",
    "    transform the dict into a padas\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_data = pd.DataFrame(data)    \n",
    "    \n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making some data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_not_used_columns(data):\n",
    "    \"\"\"\n",
    "    drop not required columns: openess indicator label, openess indicator value, total datasets label, total datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    return data.drop([1, 2, 3, 4], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# equivalences table for removing duplicated datasets: same dataset type, different names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_equivalences_table():\n",
    "    \"\"\"\n",
    "    table creation by manually entry the possible datasets types writting: added as founded in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    data_types_equivalance = {\"comma-separated-values\":\"csv\", \"sparql-query\":\"sparql\", \"tab-separated-values\":\"tsv\", \"pdf;type=pdf1x\":\"pdf\", \"rdf+xml\":\"rdf_xml\"}\n",
    "    \n",
    "    return data_types_equivalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processing each row of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_row(row_to_process, data):\n",
    "    \"\"\"\n",
    "    extract the information correspondig to a specific row for reshape it properly: <date>, <data type>, <number of datasets>\n",
    "    \"\"\"   \n",
    "    data_row = data.loc[row_to_process]\n",
    "    \n",
    "    #create a dataframe with the information extacter for each row. It will make further reshaping easier\n",
    "    return pd.DataFrame(data_row).T.reset_index().drop([\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_removing_empty_columns(data_row):\n",
    "    \"\"\"\n",
    "    remove the columns having no values. Here no value is represented by ''\n",
    "    \"\"\"\n",
    "\n",
    "    return data_row.drop([col for col in data_row if (data_row[col] == '').any()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_identification(data_row):\n",
    "    \"\"\"\n",
    "    extract the columns corresponding to the data types and the number of datasets per data type\n",
    "    \"\"\"\n",
    "\n",
    "    data_row_type_labels = data_row.columns[1:data_row.shape[1]:2]\n",
    "    data_row_value_labels = data_row.columns[2:data_row.shape[1]:2]\n",
    "    \n",
    "    return data_row_type_labels, data_row_value_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_separation(data_row, data_row_type_labels, data_row_value_labels):\n",
    "    \"\"\"\n",
    "    create 2 dataframes: 1 containing the data set type and 1 containing the value of the corresponding dataset type and reshape as row-oriented by transposing it\n",
    "    \"\"\"\n",
    "    # dataset type columns extraction\n",
    "    data_row_type = data_row[data_row_type_labels].T.reset_index()\n",
    "    data_row_type = data_row_type.drop([\"index\"], axis=1).rename(columns={0:\"dataset type\"}) # rename the column name\n",
    "    \n",
    "    # number of dataset types columns extraction\n",
    "    data_row_value = data_row[data_row_value_labels].T.reset_index()\n",
    "    data_row_value = data_row_value.drop([\"index\"], axis=1).rename(columns={0:\"number of datasets\"}) # rename the column name\n",
    "    \n",
    "    return data_row_type, data_row_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset_types_description(data_row_type):\n",
    "    \"\"\"\n",
    "    clean the row_data_type by removing unwanted text: text before the \"/\" character. It also removes any rows with 'None' value\n",
    "    \"\"\"\n",
    "\n",
    "    data_row_type = data_row_type.dropna() #remove rows having 'None' values\n",
    "    data_row_type_cleaned = data_row_type[\"dataset type\"].apply(lambda x: x.lower()).apply(lambda x: x.split(\"/\"))\n",
    "    data_row_type_cleaned = pd.DataFrame(data_row_type_cleaned)\n",
    "    data_row_type_cleaned = data_row_type_cleaned[\"dataset type\"].apply(lambda x: x[-1])\n",
    "    \n",
    "    return pd.DataFrame(data_row_type_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(data_row_type, data_row_value):\n",
    "    \"\"\"\n",
    "    concatenate the 2 created dataframes into 1 that will serve in the final datamodel for Qlik Sense\n",
    "    \"\"\"\n",
    "    \n",
    "    data_row_transformed = pd.concat([data_row_type, data_row_value], axis=1, ignore_index=True)\n",
    "\n",
    "    # rename the columns\n",
    "    data_row_transformed.rename(columns={0:\"dataset type\", 1:\"number of datasets\"}, inplace=True)\n",
    "    \n",
    "    return data_row_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_replace_equivalent_dataset_name(data_row_transformed, data_types_equivalance):\n",
    "    \"\"\"\n",
    "    find and replace dataset types names to remove equivalent names for the same types of datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    data_row_transformed.replace(data_types_equivalance, inplace=True)\n",
    "    \n",
    "    return data_row_transformed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_replace_equivalent_dataset_name(data_row_transformed, data_types_equivalance):\n",
    "    \"\"\"\n",
    "    find and replace dataset types names to remove equivalent names for the same types of datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    data_row_transformed.replace(data_types_equivalance, inplace=True)\n",
    "    \n",
    "    return data_row_transformed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_grouping(data_row_transformed):\n",
    "    \"\"\"\n",
    "    removing duplicated data: group by dataset type and summing up the number of datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    return data_row_transformed.groupby([\"dataset type\"]).sum().reset_index().rename(columns={1:\"number of datasets\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add the date column to finalise the datamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hours(data_row):\n",
    "    \"\"\"\n",
    "    remove the hours minutes and seconds in the date dimention\n",
    "    \"\"\"\n",
    "\n",
    "    date_object = datetime.datetime.strptime(data_row[0][0], '%m/%d/%Y %H:%M:%S').date()\n",
    "\n",
    "    # transform the datetime object into a string with format dd/mm/yyy\n",
    "    date = date_object.strftime('%d/%m/%Y')\n",
    "\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_proccesed_dates(dates_proccesed, current_date):\n",
    "        \"\"\"\n",
    "        create a list of already processed dates from the google sheet file containing the data source. This list will be saved on a file for reuse when updating the processed data\n",
    "        \"\"\"\n",
    "        \n",
    "        return dates_proccesed.append({\"processed dates\": current_date}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_change_detector(date_tracking, current_date, current_row): \n",
    "    \"\"\"\n",
    "    detect a month change in the dataset and add a flag to the rows of the last day of each month (flag = 1) otherwise, there is not flag (flag = 0).\n",
    "    \n",
    "    The current dataset exhibits a non-continuity of the dates, there are many days missing.\n",
    "    \"\"\"\n",
    "      \n",
    "    # extract the previous date to check if the month has changed\n",
    "    past_date = date_tracking.loc[current_row - 1][0]    \n",
    "\n",
    "    # transform the dates into a datetime object\n",
    "    past_date_datetime = datetime.datetime.strptime(past_date, \"%d/%m/%Y\")\n",
    "    current_date_datetime = datetime.datetime.strptime(current_date, \"%d/%m/%Y\")\n",
    "\n",
    "    # extract the month of the date\n",
    "    past_month = past_date_datetime.month\n",
    "    current_month = current_date_datetime.month\n",
    "\n",
    "    # detect month change\n",
    "    if current_month != past_month:\n",
    "        flag_month_change = 1\n",
    "    else:\n",
    "        flag_month_change = 0\n",
    "        \n",
    "    return flag_month_change, past_date, current_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_formatting(length_data_row, date):\n",
    "    \"\"\"\n",
    "    create a dataframe with the date having equal length that row_data_type_labels and row_data_value_labels\n",
    "    \"\"\"\n",
    "    \n",
    "    date_list = [[date, 0] for i in range(length_data_row)]\n",
    "           \n",
    "    return pd.DataFrame(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_data_merge(df_date, data_row_transformed):\n",
    "    \"\"\"\n",
    "    for the current processing row: add the date colum to the dataframe containing the cleaned version of the data types and the dataset number per type\n",
    "    \"\"\"\n",
    "\n",
    "    data_row_transformed = pd.concat([df_date, data_row_transformed], axis=1, ignore_index=True)\n",
    "\n",
    "    # rename the columns, sort values by number of datasets type and remove a self-created \"index\" column\n",
    "    data_row_transformed.rename(columns={0:\"date\", 1:\"last day of the month\", 2:\"dataset type\", 3:\"number of datasets\"}, inplace=True)\n",
    "    data_row_transformed = data_row_transformed.sort_values(by=[\"number of datasets\"], ascending=False).reset_index().drop([\"index\"], axis=1)\n",
    "    \n",
    "    return data_row_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_flag_for_month_change(data_processed, past_date):\n",
    "    \"\"\"\n",
    "    add a flag in case of month change detected - month_change_flag = 1    \n",
    "    \"\"\"\n",
    "    \n",
    "    # add a flag=1 to a previous date if there is month change\n",
    "    data_processed.loc[data_processed[\"date\"] == past_date, \"last day of the month\"] = 1\n",
    "    \n",
    "    return data_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update the data and the last update info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_backup(directory_path, processed_dates_file, processed_data_file): \n",
    "    \"\"\"\n",
    "    created a copy of the previous files before append data to it renaming the file as <filename>_backup:\n",
    "    * processed_dates_file {string}: filename containing the dates processed so far\n",
    "    * data_file {string}: filename containing the data processed so far    \n",
    "    \n",
    "    if the file doesn't exist it does anything    \n",
    "    \"\"\"\n",
    "    \n",
    "    # creates a backup of the file containing the dates processed so far\n",
    "    dates_file = directory_path + processed_dates_file\n",
    "    dates_file_backup = directory_path + str.split(processed_dates_file, \".\")[0] + \"_backup.\" + str.split(processed_dates_file, \".\")[1]\n",
    "    if pathlib.Path(dates_file).exists():\n",
    "        shutil.copyfile(dates_file, dates_file_backup)        \n",
    "    \n",
    "     # creates a backup of the file containing the data processed so far\n",
    "    data_file = directory_path + processed_data_file\n",
    "    data_file_backup = directory_path + str.split(processed_data_file, \".\")[0] + \"_backup.\" + str.split(processed_data_file, \".\")[1]\n",
    "    if pathlib.Path(data_file).exists():\n",
    "        shutil.copyfile(data_file, data_file_backup)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data_and_info_update(data_processed, date_processed, directory_path, processed_dates_file, data_file): \n",
    "    \"\"\"\n",
    "    update the processed data into a CSV file by appending the procesed data to existing one in the file: datasets_formats_processed.csv\n",
    "    update the processed dates CSV file by appending the processed dates to the existing ones in the file: processed_dates_update.csv\n",
    "    \n",
    "    IMPORTANT: put attention to the date format in the \"processed_dates_update.csv\" file. The format doesn't match the format used in google sheets. You will need to change the date format in this file before\n",
    "                going for updates\n",
    "    \"\"\"   \n",
    "    \n",
    "    data_backup(directory_path, processed_dates_file, data_file)\n",
    "    \n",
    "    # save the processed data into a CSV file with headers\n",
    "    file_path_data = directory_path + data_file  # my home laptop\n",
    "    data_processed.to_csv(file_path_data, mode=\"a\", index=False, header=False)\n",
    "    \n",
    "    # save the last row processed info (google sheet row number and last date present in this row) into a CSV file with headers\n",
    "    file_path_update = directory_path + processed_dates_file  # my home laptop\n",
    "    date_processed.to_csv(file_path_update, mode=\"a\", index=False, header=False)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_execute(data_cleaned):\n",
    "    \"\"\"\n",
    "    Enables the execution of the script depending of the dates in the google sheet file and the current date. \n",
    "    It prevents the script runs when there isn't a change in the month making impossible to detect a month change.\n",
    "    \"\"\"\n",
    "    \n",
    "    before_last_date = data_cleaned[0][len(data_cleaned[0]) - 2]\n",
    "    before_last_date_month = int(before_last_date.split(\"/\")[0])\n",
    "\n",
    "    last_date = data_cleaned[0][len(data_cleaned[0]) - 1]\n",
    "    last_date_month = int(last_date.split(\"/\")[0])\n",
    "\n",
    "    today_month = dt.today().month\n",
    "\n",
    "    if (last_date_month >= before_last_date_month) & (today_month == last_date_month):\n",
    "        print(\"script execution enabled...program continued\")\n",
    "        return 0 # continues the execution of the script\n",
    "    else:\n",
    "        print(\"script execution stopped. Not the right day for executing it...program terminated\")\n",
    "        return 1 # terminates the execution of the script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --> main function <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # global variables\n",
    "    SHEET = \"ODP OPENNESS INDICATOR_local_4\"\n",
    "    WORKSHEET = \"History\"\n",
    "    # DIRECTORY_PATH = \"D:\\\\Dropbox\\\\Programming\\\\Python\\\\datasets files formats ODP\\\\\"  # my office laptop\n",
    "    DIRECTORY_PATH = \"D:\\\\Dropbox\\\\Programming\\\\Python\\\\ODP_datasets_distributions\\\\\"  # my home laptop\n",
    "    PROCESSED_DATES_FILE = \"processed_dates_update.csv\"\n",
    "    PROCESSED_DATA_FILE = \"datasets_formats_processed.csv\"\n",
    "    FIRST_COLUMN = \"A\" # first column in the google sheet file\n",
    "    LAST_COLUMN = \"EM\" # last columnn in the google sheet file. To change when the data in the google sheet will go beyond the EM column\n",
    "\n",
    "    # set connection to google drive\n",
    "    google_client = set_google_drive_access_scope()\n",
    "\n",
    "    # acquire the data\n",
    "    data_types = access_google_sheet(SHEET, WORKSHEET, google_client, PROCESSED_DATES_FILE, FIRST_COLUMN, LAST_COLUMN)[0]\n",
    "    if data_types:\n",
    "        raw_data = transform_into_dataframe(data_types)\n",
    "    else: \n",
    "        print(\"No new data to read --> EXIT\")\n",
    "        sys.exit(0)  # terminates the program with no errors\n",
    "\n",
    "    total_rows = raw_data.shape[0]\n",
    "    print(f'the size of the imported data is: {raw_data.shape}\\n')\n",
    "\n",
    "    # clean the data\n",
    "    data_cleaned = drop_not_used_columns(raw_data)\n",
    "\n",
    "    # enable or disable the execution of the script according to the current date and the data available in the google sheet\n",
    "    if can_execute(data_cleaned):\n",
    "        return\n",
    "    \n",
    "    # load the table containing equivalent names for the same dataset. It'll be used to have the same name for the same dataset type\n",
    "    datasets_type_equivalences = load_equivalences_table()\n",
    "\n",
    "    # process all the rows of the dataset\n",
    "    #total_rows = 10\n",
    "    for current_row in tqdm(range(total_rows), desc=\"data rows processing\"): #tqdm_notebook\n",
    "        # process each row\n",
    "        row_data = extract_row(current_row, data_cleaned)\n",
    "\n",
    "        # remove empty columns in the extracted row\n",
    "        row_data_clean = row_removing_empty_columns(row_data)\n",
    "\n",
    "        # identification of the columns related to the dataset formats and the columns related to the number of dataset formats\n",
    "        row_data_type_labels, row_data_value_labels = columns_identification(row_data_clean)\n",
    "\n",
    "        # separation of the columns related to the dataset formats and the columns related to the number of dataset formats\n",
    "        row_data_type, row_data_value = columns_separation(row_data_clean, row_data_type_labels, row_data_value_labels)\n",
    "\n",
    "        # clean the dataset type desciption column by filtering out the dataset type\n",
    "        row_data_type_clean = clean_dataset_types_description (row_data_type)\n",
    "\n",
    "        # merge dataset types and dataset values into a single dataframe\n",
    "        row_data_transformed = merge_columns(row_data_type_clean, row_data_value)\n",
    "\n",
    "        # find and replace equivalent names for the same dataset type\n",
    "        row_data_transformed_cleaned = find_and_replace_equivalent_dataset_name(row_data_transformed, datasets_type_equivalences)\n",
    "\n",
    "        # combine together same dataset types and sum up the number of datasets per data type\n",
    "        row_data_transformed_cleaned = data_grouping(row_data_transformed_cleaned)\n",
    "\n",
    "        # remove hour info from the date\n",
    "        date_clean = remove_hours(row_data)       \n",
    "\n",
    "        # add the current date to the list of already processed dates\n",
    "        if current_row == 0:\n",
    "            date_tracking = pd.DataFrame({\"processed dates\": [date_clean]})        \n",
    "        else:\n",
    "            date_tracking = track_proccesed_dates(date_tracking, date_clean)\n",
    "            # detect a change of month \n",
    "            month_change_flag, past_date, current_date = month_change_detector(date_tracking, date_clean, current_row)\n",
    "        \n",
    "        # generate a new date column with a month change flag column ready to add to the cleaned row dataset\n",
    "        df_date = date_formatting(row_data_transformed_cleaned.shape[0], date_clean)\n",
    "\n",
    "        # add the date info to the dataframe containing he dataset types and the number of dataset types and make some formatting\n",
    "        row_data_final = row_data_merge(df_date, row_data_transformed_cleaned)\n",
    "\n",
    "         # append the processed data\n",
    "        if current_row == 0:\n",
    "            data_processed = row_data_final\n",
    "        else:\n",
    "            data_processed = data_processed.append(row_data_final, ignore_index=True)\n",
    "            # change the \"last day of the month\" column vaue to 0 --> 1\n",
    "            if month_change_flag:\n",
    "                data_processed = add_flag_for_month_change(data_processed, past_date)\n",
    "\n",
    "    # save the processed data into a CSV file\n",
    "    update_data_and_info_update(data_processed, date_tracking, DIRECTORY_PATH, PROCESSED_DATES_FILE, PROCESSED_DATA_FILE) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --> Execution of the data update starts here <--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of the imported data is: (42, 91)\n",
      "\n",
      "script execution enabled...program continued\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0803d05abde7493092b334f39d3b036e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='data rows processing', max=42.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
